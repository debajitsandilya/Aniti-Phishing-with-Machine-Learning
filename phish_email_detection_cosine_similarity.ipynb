{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the Input Text for both Good and Bad content used for Comparison\n",
    "good_email  = open(\"ideal_email.txt\", \"r\")\n",
    "d1 = good_email.readlines() # read the good content\n",
    "\n",
    "######################################################\n",
    "fake_email = open(\"spam_email_text.txt\" , \"r\")\n",
    "d2 = fake_email.readlines() # read the bad content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert to string  \n",
    "\n",
    "def listToString(s):  \n",
    "    str1 = \"\"  # initialize an empty string \n",
    "     \n",
    "    for ele in s: #traverse in the string   \n",
    "        str1 += ele   \n",
    " \n",
    "    return str1 #return string  \n",
    "        \n",
    "        \n",
    "#Call the function and Convert  \n",
    "d1= listToString(d1) \n",
    "d2= listToString(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add strings from good and bad to list combined way\n",
    "documents= [d1, d2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"ï»¿Hello\\nGreat news, your iiNet broadband connection is now active!\\nHere are a few things you should know to get up. and running with your new\\nservice...\\nFirstly, if you've bundled your internet service with phone or any other product,\\nthey will be activated shortly.\\nTo get your broadband service online, you'll need a broadband modem or router.\\nYou may have already purchased one from us during your application, in which\\ncase it will be dispatched today. If you're supplying your own hardware then you\\ncan get online straight away.\\nAlternatively, you can purchase hardware at any time via Toolbox at\\nhttps://toolbox.iinet.net.au.\\nQuick Configuration Information\\nYou'll need to use your usernameÂ©iinet.net.au to configure your account and\\nconnect to your new superfast broadband.\\n1. Username:\\n2. Password:\\n3. Broadband phone number:\\n4. IP address(es): dynamic\\n5. Email address:\\n6. Billing email address:\\n7. Account type: Home-3\\n\",\n",
       " 'ï»¿Subject: t9 New verion available, The 911 upgrade is recommended for all customers t9\\niinet\\nconnect better\\nDear iiNet Customer,\\nStarting March 11H-1 2019, older versions of iiNet web-mail will be discontinued. Please\\nenhance to the newest version to continue u                                                  ion.\\nhttps://\\nDo not worry, you can enhance to new versi                                                   toolb oy3 iinetnetauloginreturn2fiihelpada                          This link will\\nminternetlegacysettingsmeebly.comi\\nonly be available for 48 hours\\nClick to follow link\\nhttps://toolbox3.iinet netuuiloqin?ReturnUrl=\\nT-is                                                                                         oe available for 48 hours\\nWhat\\'s different in the new version?\\nT-is ve:rsic-                                                                                i-cludes an upgraded interface, enhancements to many\\nexis-i-g fea-ores: and addi-ic nal features that were not previously available in Web-mail\\nClassic.                                                                                     â€”a- c-a-ge can be difficult and we want to make the transition as\\ncmccâ€”as ocssi de.\\n,1101111111.111#\"-\\n-.creased email storage with up to 1TB\\nFas-er oaâ€¢e a-d email bad times\\n1..,1iore functional search feature\\nIncreased security\\nSupports the latest web browsers and plug-ins (1E, Firefox and Safari)\\nWe hope you\\'ll find these updates improve your customer experience and reinforce\\nour commitment to your data privacy and security.\\nThank you for choosing iiNet\\nCraig Levy\\nChief Operating Officer\\niino\\nconnect b\\nCopyright 0 UNet Limited.\\nACN 068 623 937. All rights reserved.\\n']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Piyush's\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#importing packages for word processing\n",
    "import nltk, string, numpy\n",
    "nltk.download('punkt') \n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "def StemTokens(tokens):\n",
    "     return [stemmer.stem(token) for token in tokens]\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "def StemNormalize(text):\n",
    "     return StemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict))) # text normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Piyush's\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#import the wordnet to bring in English dictionary form\n",
    "nltk.download('wordnet') \n",
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "def LemTokens(tokens):\n",
    "     return [lemmer.lemmatize(token) for token in tokens]\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "def LemNormalize(text):\n",
    "     return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict))) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x178 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 187 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "LemVectorizer = CountVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "LemVectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'©': 114, 'ft': 50, 'bd': 11, 'wesrbziczvrygradionovasondasfmcomwwwcontxmlrpcagvubmluz3naz3d1lmvkdqe': 111, '5': 1, 'read': 84, 'later': 62, '38': 0, 'eval': 42, 'gzinflate': 54, 'base64': 10, 'yaragenerstor': 112, 'aut': 9, 'ticket': 102, 'eventin': 43, 'github': 52, 'ey': 45, 'ddecodephp': 29, 'dec': 30, 'syrian': 99, 'united': 105, 'st': 97, 'goose': 53, 'qe': 83, 'mvaccount': 68, 'ssi': 96, 'card': 15, 'pf': 76, 'travel': 103, 'brewards': 12, 'rj': 90, 'business': 13, 'dc': 28, 'ec': 36, 'ce': 17, 'additional': 3, 'security': 93, 'enter': 41, 'following': 47, 'information': 60, 'confirm': 23, 'identity': 59, 'proceed': 78, 'verification': 108, 'process': 79, 'verify': 109, 'user': 106, 'id': 58, 'password': 74, 'provide': 81, 'cid': 19, 'number': 71, 'located': 65, 'g': 51, 'dia': 31, 'code': 20, 'csc': 27, 'digit': 33, 'signature': 95, 'strip': 98, 'diats': 32, '‘': 116, 'email': 40, 'address': 4, 'ee': 38, 'month': 67, 'year': 113, 'v': 107, 'cancel': 14, 'continue': 24, 'complete': 21, 'term': 101, 'condition': 22, '—': 115, 'account': 2, 'reward': 89, 'q': 82, 'help': 56, 'american': 7, 'express': 44, 'investor': 61, 'relation': 86, 'partner': 73, 'don': 34, '’': 117, 't': 100, 'hit': 57, 'road': 91, 'carry': 16, 'membership': 66, 'prk': 77, 'hae': 55, 'nn': 70, 'eek': 39, 'ub': 104, 'ar': 8, 'pent': 75, 'ea': 35, 'remember': 87, 'ain': 6, 'forgot': 48, 'create': 25, 'new': 69, 'online': 72, 'received': 85, 'visit': 110, 'center': 18, 'product': 80, 'service': 94, 'link': 64, 'like': 63, 'credit': 26, 'agreement': 5, 'free': 49, 'score': 92, 'report': 88, 'financial': 46, 'education': 37}\n"
     ]
    }
   ],
   "source": [
    "#LemVectorizer object \n",
    "#print(LemVectorizer)\n",
    "print(LemVectorizer.vocabulary_) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 0 0 0 0 1 0 1 1 0 1 1 0 1 0 0 2 0 1 1 0 0 3 1 1 0 1 0 0 0 1 5 0\n",
      "  1 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1 2 0 0 0 0 0 0 0 0 1 0\n",
      "  0 1 2 1 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 0 1 0 2 0 2 0 1 1\n",
      "  0 0 0 0 0 2 0 0 1 2 0 0 0 1 1 1 1 0 0 0 0 1 1 0 0 0 3 1 0 0 1 1 1 0 0 0\n",
      "  1 0 1 1 0 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 2 1 1 1 1 0 0 1 0]\n",
      " [1 1 0 1 1 1 1 1 0 1 0 0 2 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 4 0 1 1 1 0 0 1\n",
      "  0 1 0 1 1 1 1 1 1 0 0 2 0 1 1 1 1 3 1 1 1 1 1 0 0 2 2 1 1 1 1 1 2 1 0 1\n",
      "  1 0 0 0 1 2 1 1 0 1 4 1 1 1 1 0 1 0 1 0 0 1 1 1 2 1 1 1 0 1 0 1 3 1 0 0\n",
      "  1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 0 1 1 1 1 0 0 1 1 2 0 0 1 1 0 0 0 1 2 1\n",
      "  0 2 1 0 1 0 1 0 1 1 1 1 1 0 0 0 1 1 1 3 1 1 2 1 1 1 0 0 1 0 1 1 1 2]]\n"
     ]
    }
   ],
   "source": [
    "#transfrom the LemVectorizer \n",
    "tf_matrix = LemVectorizer.transform(documents).toarray()\n",
    "print(tf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 178)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The idf for terms that appear in one document: 1.916290731874155\n",
      "The idf for terms that appear in two documents: 1.5108256237659907\n"
     ]
    }
   ],
   "source": [
    "#import the inverse document frequency\n",
    "import math\n",
    "def idf(n,df):\n",
    "    result = math.log((n+1.0)/(df+1.0)) + 1\n",
    "    return result\n",
    "print(\"The idf for terms that appear in one document: \" + str(idf(4,1)))\n",
    "print(\"The idf for terms that appear in two documents: \" + str(idf(4,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.08976198 0.         0.         0.\n",
      "  0.         0.         0.08976198 0.         0.08976198 0.08976198\n",
      "  0.         0.08976198 0.08976198 0.         0.08976198 0.\n",
      "  0.         0.17952396 0.         0.08976198 0.08976198 0.\n",
      "  0.         0.26928594 0.08976198 0.08976198 0.         0.08976198\n",
      "  0.         0.         0.         0.08976198 0.4488099  0.\n",
      "  0.08976198 0.         0.08976198 0.         0.         0.\n",
      "  0.         0.         0.         0.08976198 0.08976198 0.06386639\n",
      "  0.08976198 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.08976198\n",
      "  0.08976198 0.12773278 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.08976198 0.\n",
      "  0.         0.08976198 0.17952396 0.08976198 0.         0.\n",
      "  0.         0.         0.08976198 0.         0.06386639 0.\n",
      "  0.         0.         0.         0.08976198 0.         0.08976198\n",
      "  0.         0.08976198 0.08976198 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.08976198 0.\n",
      "  0.17952396 0.         0.12773278 0.         0.08976198 0.08976198\n",
      "  0.         0.         0.         0.         0.         0.17952396\n",
      "  0.         0.         0.08976198 0.17952396 0.         0.\n",
      "  0.         0.08976198 0.08976198 0.08976198 0.08976198 0.\n",
      "  0.         0.         0.         0.08976198 0.08976198 0.\n",
      "  0.         0.         0.26928594 0.08976198 0.         0.\n",
      "  0.08976198 0.08976198 0.08976198 0.         0.         0.\n",
      "  0.08976198 0.         0.06386639 0.08976198 0.         0.08976198\n",
      "  0.         0.08976198 0.06386639 0.         0.         0.\n",
      "  0.         0.08976198 0.08976198 0.08976198 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.12773278 0.08976198 0.08976198 0.06386639 0.08976198\n",
      "  0.         0.         0.06386639 0.        ]\n",
      " [0.07269368 0.07269368 0.         0.07269368 0.07269368 0.07269368\n",
      "  0.07269368 0.07269368 0.         0.07269368 0.         0.\n",
      "  0.14538736 0.         0.         0.07269368 0.         0.07269368\n",
      "  0.07269368 0.         0.07269368 0.         0.         0.07269368\n",
      "  0.07269368 0.         0.         0.         0.29077472 0.\n",
      "  0.07269368 0.07269368 0.07269368 0.         0.         0.07269368\n",
      "  0.         0.07269368 0.         0.07269368 0.07269368 0.07269368\n",
      "  0.07269368 0.07269368 0.07269368 0.         0.         0.10344431\n",
      "  0.         0.07269368 0.07269368 0.07269368 0.07269368 0.21808104\n",
      "  0.07269368 0.07269368 0.07269368 0.07269368 0.07269368 0.\n",
      "  0.         0.10344431 0.14538736 0.07269368 0.07269368 0.07269368\n",
      "  0.07269368 0.07269368 0.14538736 0.07269368 0.         0.07269368\n",
      "  0.07269368 0.         0.         0.         0.07269368 0.14538736\n",
      "  0.07269368 0.07269368 0.         0.07269368 0.20688861 0.07269368\n",
      "  0.07269368 0.07269368 0.07269368 0.         0.07269368 0.\n",
      "  0.07269368 0.         0.         0.07269368 0.07269368 0.07269368\n",
      "  0.14538736 0.07269368 0.07269368 0.07269368 0.         0.07269368\n",
      "  0.         0.07269368 0.15516646 0.07269368 0.         0.\n",
      "  0.07269368 0.07269368 0.07269368 0.07269368 0.07269368 0.\n",
      "  0.07269368 0.07269368 0.         0.         0.07269368 0.07269368\n",
      "  0.07269368 0.         0.         0.         0.         0.07269368\n",
      "  0.07269368 0.07269368 0.07269368 0.         0.         0.07269368\n",
      "  0.07269368 0.14538736 0.         0.         0.07269368 0.07269368\n",
      "  0.         0.         0.         0.07269368 0.14538736 0.07269368\n",
      "  0.         0.14538736 0.05172215 0.         0.07269368 0.\n",
      "  0.07269368 0.         0.05172215 0.07269368 0.07269368 0.07269368\n",
      "  0.07269368 0.         0.         0.         0.07269368 0.07269368\n",
      "  0.07269368 0.21808104 0.07269368 0.07269368 0.14538736 0.07269368\n",
      "  0.07269368 0.05172215 0.         0.         0.05172215 0.\n",
      "  0.07269368 0.07269368 0.05172215 0.14538736]]\n"
     ]
    }
   ],
   "source": [
    "#Referred from https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html\n",
    "#We found the TfidTransformer and the way to use it for generating tfidf matrix\n",
    "#tf-idf weight is a weight often used in information retrieval and text mining\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidfTran = TfidfTransformer(norm=\"l2\")\n",
    "tfidfTran.fit(tf_matrix)\n",
    "tfidf_matrix = tfidfTran.transform(tf_matrix)\n",
    "print(tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 116)\t0.17020925160727626\n",
      "  (0, 114)\t0.17020925160727626\n",
      "  (0, 113)\t0.08510462580363813\n",
      "  (0, 112)\t0.08510462580363813\n",
      "  (0, 111)\t0.08510462580363813\n",
      "  (0, 109)\t0.08510462580363813\n",
      "  (0, 108)\t0.08510462580363813\n",
      "  (0, 107)\t0.08510462580363813\n",
      "  (0, 106)\t0.06055264219130546\n",
      "  (0, 105)\t0.08510462580363813\n",
      "  (0, 103)\t0.06055264219130546\n",
      "  (0, 102)\t0.08510462580363813\n",
      "  (0, 101)\t0.08510462580363813\n",
      "  (0, 99)\t0.08510462580363813\n",
      "  (0, 98)\t0.08510462580363813\n",
      "  (0, 97)\t0.08510462580363813\n",
      "  (0, 96)\t0.08510462580363813\n",
      "  (0, 95)\t0.08510462580363813\n",
      "  (0, 93)\t0.24221056876522185\n",
      "  (0, 90)\t0.08510462580363813\n",
      "  (0, 84)\t0.08510462580363813\n",
      "  (0, 83)\t0.08510462580363813\n",
      "  (0, 81)\t0.17020925160727626\n",
      "  (0, 79)\t0.08510462580363813\n",
      "  (0, 78)\t0.08510462580363813\n",
      "  :\t:\n",
      "  (1, 58)\t0.15408768889914773\n",
      "  (1, 57)\t0.10828243516838894\n",
      "  (1, 56)\t0.10828243516838894\n",
      "  (1, 55)\t0.10828243516838894\n",
      "  (1, 49)\t0.10828243516838894\n",
      "  (1, 48)\t0.10828243516838894\n",
      "  (1, 46)\t0.10828243516838894\n",
      "  (1, 44)\t0.10828243516838894\n",
      "  (1, 39)\t0.10828243516838894\n",
      "  (1, 37)\t0.10828243516838894\n",
      "  (1, 35)\t0.10828243516838894\n",
      "  (1, 34)\t0.10828243516838894\n",
      "  (1, 26)\t0.3248473055051668\n",
      "  (1, 25)\t0.10828243516838894\n",
      "  (1, 23)\t0.07704384444957386\n",
      "  (1, 18)\t0.10828243516838894\n",
      "  (1, 16)\t0.10828243516838894\n",
      "  (1, 15)\t0.3852192222478693\n",
      "  (1, 13)\t0.15408768889914773\n",
      "  (1, 8)\t0.10828243516838894\n",
      "  (1, 7)\t0.10828243516838894\n",
      "  (1, 6)\t0.10828243516838894\n",
      "  (1, 5)\t0.10828243516838894\n",
      "  (1, 3)\t0.07704384444957386\n",
      "  (1, 2)\t0.21656487033677788\n",
      "  (116, 0)\t0.17020925160727626\n",
      "  (114, 0)\t0.17020925160727626\n",
      "  (113, 0)\t0.08510462580363813\n",
      "  (112, 0)\t0.08510462580363813\n",
      "  (111, 0)\t0.08510462580363813\n",
      "  (109, 0)\t0.08510462580363813\n",
      "  (108, 0)\t0.08510462580363813\n",
      "  (107, 0)\t0.08510462580363813\n",
      "  (106, 0)\t0.06055264219130546\n",
      "  (105, 0)\t0.08510462580363813\n",
      "  (103, 0)\t0.06055264219130546\n",
      "  (102, 0)\t0.08510462580363813\n",
      "  (101, 0)\t0.08510462580363813\n",
      "  (99, 0)\t0.08510462580363813\n",
      "  (98, 0)\t0.08510462580363813\n",
      "  (97, 0)\t0.08510462580363813\n",
      "  (96, 0)\t0.08510462580363813\n",
      "  (95, 0)\t0.08510462580363813\n",
      "  (93, 0)\t0.24221056876522185\n",
      "  (90, 0)\t0.08510462580363813\n",
      "  (84, 0)\t0.08510462580363813\n",
      "  (83, 0)\t0.08510462580363813\n",
      "  (81, 0)\t0.17020925160727626\n",
      "  (79, 0)\t0.08510462580363813\n",
      "  (78, 0)\t0.08510462580363813\n",
      "  :\t:\n",
      "  (58, 1)\t0.15408768889914773\n",
      "  (57, 1)\t0.10828243516838894\n",
      "  (56, 1)\t0.10828243516838894\n",
      "  (55, 1)\t0.10828243516838894\n",
      "  (49, 1)\t0.10828243516838894\n",
      "  (48, 1)\t0.10828243516838894\n",
      "  (46, 1)\t0.10828243516838894\n",
      "  (44, 1)\t0.10828243516838894\n",
      "  (39, 1)\t0.10828243516838894\n",
      "  (37, 1)\t0.10828243516838894\n",
      "  (35, 1)\t0.10828243516838894\n",
      "  (34, 1)\t0.10828243516838894\n",
      "  (26, 1)\t0.3248473055051668\n",
      "  (25, 1)\t0.10828243516838894\n",
      "  (23, 1)\t0.07704384444957386\n",
      "  (18, 1)\t0.10828243516838894\n",
      "  (16, 1)\t0.10828243516838894\n",
      "  (15, 1)\t0.3852192222478693\n",
      "  (13, 1)\t0.15408768889914773\n",
      "  (8, 1)\t0.10828243516838894\n",
      "  (7, 1)\t0.10828243516838894\n",
      "  (6, 1)\t0.10828243516838894\n",
      "  (5, 1)\t0.10828243516838894\n",
      "  (3, 1)\t0.07704384444957386\n",
      "  (2, 1)\t0.21656487033677788\n",
      "[[1.         0.27058208]\n",
      " [0.27058208 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "#cosine similarity using tfidf_matrix0 \n",
    "\n",
    "cos_similarity_matrix = (tfidf_matrix * tfidf_matrix.T).toarray()\n",
    "print(cos_similarity_matrix)\n",
    "import numpy as np\n",
    "np.savetxt('C:\\\\xampp\\\\htdocs\\\\cyber_project\\\\logs_cosine_sim.txt' , cos_similarity_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\installations\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.27058208],\n",
       "       [0.27058208, 1.        ]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using TfidVectorizer and using stop_words in English dictionary\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "def cos_similarity(textlist):\n",
    "    tfidf = TfidfVec.fit_transform(textlist)\n",
    "    return (tfidf * tfidf.T).toarray()\n",
    "cos_similarity(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another approach to perform text comparison from images. Since HTML access could not be possbile sometimes, we tried using the same approach over fake and real images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "© > © ft BD wesrbziczvryg.radionovasondasfm.com/~wwwcont/xmlrpc/aGVubmluZ3NAZ3d1LmVkdQ==?e=[5 Read Later 38 eval gzinflate base64... @ YaraGenerstor- Aut... [( Tickets and Eventin.. ) GitHub [EY DDecode-PHP Dec... Syrian 5 United stgeese] == Qe mvaccount SSI caRDs PF TRAVEL BREWARDS [RJ BUSINESSDC eC a Ce For additional security, please enter the following information so thatwe may confirm your identity and proceed with the verification process.                                     Verify Your CardUSER ID: PASSWORD:For your security, please provide the CID numberCID located on the front of your card.CARD ID NUMBER G dias)Please also provide the card security code (CSC)CSC from the back of your card. Its the last threedigits in the signature strip.CARD SECURITY CODE @diats)‘CARD NUMBER: EMAIL ADDRESS:ee Month [| [Year |vCANCEL CONTINUE‘Complete Terms and Conditions.\n"
     ]
    }
   ],
   "source": [
    "#image extraction from pytesseract processing library\n",
    "\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "im_1 = Image.open(\"fake_amex_site.png\")\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "#extracting fake site content\n",
    "text_1 = pytesseract.image_to_string(im_1, lang = 'eng')\n",
    "text_1= text_1.replace(\"\\n\",\"\\t\")\n",
    "print(text_1.replace(\"\\t\" , \"\"))\n",
    "im_2 = Image.open(\"real_amex_site.png\")\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "— My Account Cards Travel Rewards Business Q HelpABOUTAbout American ExpressInvestor RelationsUSER ID FROM OUR PARTNERSDON’T HITTHE ROADWITHOUT ITCarry your membershipPrk hae nNPASSWORDeek ub ar)Pent ea)Remember Me  ain)Forgot User ID or Password?Create New Online AccountConfirm Card ReceivedVisit Our Security CenterPRODUCTS & SERVICES LINKS YOU MAY LIKE ADDITIONAL INFORMATIONCredit Cards Membership Rewards Card AgreementsBusiness Credit Cards Free Credit Score & Report Financial Education\n"
     ]
    }
   ],
   "source": [
    "#extracting fake site content\n",
    "text_2 = pytesseract.image_to_string(im_2, lang = 'eng')\n",
    "text_2 = text_2.replace(\"\\n\",\"\\t\")\n",
    "print(text_2.replace(\"\\t\" , \"\"))\n",
    "#adding good and bad content to a list\n",
    "documents = [text_1, text_2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Piyush's\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk, string, numpy\n",
    "nltk.download('punkt') # first-time use only\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "def StemTokens(tokens):\n",
    "     return [stemmer.stem(token) for token in tokens]\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "def StemNormalize(text):\n",
    "     return StemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Piyush's\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet') # first-time use only\n",
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "def LemTokens(tokens):\n",
    "     return [lemmer.lemmatize(token) for token in tokens]\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "def LemNormalize(text):\n",
    "     return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\installations\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<2x118 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 128 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "LemVectorizer = CountVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "LemVectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'©': 114, 'ft': 50, 'bd': 11, 'wesrbziczvrygradionovasondasfmcomwwwcontxmlrpcagvubmluz3naz3d1lmvkdqe': 111, '5': 1, 'read': 84, 'later': 62, '38': 0, 'eval': 42, 'gzinflate': 54, 'base64': 10, 'yaragenerstor': 112, 'aut': 9, 'ticket': 102, 'eventin': 43, 'github': 52, 'ey': 45, 'ddecodephp': 29, 'dec': 30, 'syrian': 99, 'united': 105, 'st': 97, 'goose': 53, 'qe': 83, 'mvaccount': 68, 'ssi': 96, 'card': 15, 'pf': 76, 'travel': 103, 'brewards': 12, 'rj': 90, 'business': 13, 'dc': 28, 'ec': 36, 'ce': 17, 'additional': 3, 'security': 93, 'enter': 41, 'following': 47, 'information': 60, 'confirm': 23, 'identity': 59, 'proceed': 78, 'verification': 108, 'process': 79, 'verify': 109, 'user': 106, 'id': 58, 'password': 74, 'provide': 81, 'cid': 19, 'number': 71, 'located': 65, 'g': 51, 'dia': 31, 'code': 20, 'csc': 27, 'digit': 33, 'signature': 95, 'strip': 98, 'diats': 32, '‘': 116, 'email': 40, 'address': 4, 'ee': 38, 'month': 67, 'year': 113, 'v': 107, 'cancel': 14, 'continue': 24, 'complete': 21, 'term': 101, 'condition': 22, '—': 115, 'account': 2, 'reward': 89, 'q': 82, 'help': 56, 'american': 7, 'express': 44, 'investor': 61, 'relation': 86, 'partner': 73, 'don': 34, '’': 117, 't': 100, 'hit': 57, 'road': 91, 'carry': 16, 'membership': 66, 'prk': 77, 'hae': 55, 'nn': 70, 'eek': 39, 'ub': 104, 'ar': 8, 'pent': 75, 'ea': 35, 'remember': 87, 'ain': 6, 'forgot': 48, 'create': 25, 'new': 69, 'online': 72, 'received': 85, 'visit': 110, 'center': 18, 'product': 80, 'service': 94, 'link': 64, 'like': 63, 'credit': 26, 'agreement': 5, 'free': 49, 'score': 92, 'report': 88, 'financial': 46, 'education': 37}\n"
     ]
    }
   ],
   "source": [
    "#generated vocabulary\n",
    "print(LemVectorizer.vocabulary_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 0 1 1 0 0 0 0 1 1 1 1 1 1 8 0 1 0 2 2 1 1 1 1 0 0 2 1 1 1 1 1 1 0 0\n",
      "  1 0 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 1 0 0 0 2 1 1 0 1 0 0 1 0 1 1 0 0 3\n",
      "  0 0 1 0 1 0 1 1 0 2 0 1 1 0 0 0 0 0 1 0 0 4 0 1 1 1 1 1 0 1 1 1 0 1 1 1\n",
      "  1 1 0 1 1 1 2 0 2 0]\n",
      " [0 0 2 1 0 1 1 1 1 0 0 0 0 2 0 5 1 0 1 0 0 0 0 1 0 1 3 0 0 0 0 0 0 0 1 1\n",
      "  0 1 0 1 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 2 0 1 1 0 1 1 0 2 0 0 1 1 0\n",
      "  1 1 2 1 0 1 0 0 1 0 1 0 0 1 1 1 1 2 0 1 1 1 1 0 0 0 0 0 1 0 0 1 1 0 2 0\n",
      "  0 0 1 0 0 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "tf_matrix = LemVectorizer.transform(documents).toarray()\n",
    "print(tf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 118)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.08510463 0.17020925 0.         0.06055264 0.08510463 0.\n",
      "  0.         0.         0.         0.08510463 0.08510463 0.08510463\n",
      "  0.08510463 0.06055264 0.08510463 0.48442114 0.         0.08510463\n",
      "  0.         0.17020925 0.17020925 0.08510463 0.08510463 0.06055264\n",
      "  0.08510463 0.         0.         0.17020925 0.08510463 0.08510463\n",
      "  0.08510463 0.08510463 0.08510463 0.08510463 0.         0.\n",
      "  0.08510463 0.         0.08510463 0.         0.08510463 0.08510463\n",
      "  0.08510463 0.08510463 0.         0.08510463 0.         0.08510463\n",
      "  0.         0.         0.08510463 0.08510463 0.08510463 0.08510463\n",
      "  0.08510463 0.         0.         0.         0.12110528 0.08510463\n",
      "  0.06055264 0.         0.08510463 0.         0.         0.08510463\n",
      "  0.         0.08510463 0.08510463 0.         0.         0.25531388\n",
      "  0.         0.         0.06055264 0.         0.08510463 0.\n",
      "  0.08510463 0.08510463 0.         0.17020925 0.         0.08510463\n",
      "  0.08510463 0.         0.         0.         0.         0.\n",
      "  0.08510463 0.         0.         0.24221057 0.         0.08510463\n",
      "  0.08510463 0.08510463 0.08510463 0.08510463 0.         0.08510463\n",
      "  0.08510463 0.06055264 0.         0.08510463 0.06055264 0.08510463\n",
      "  0.08510463 0.08510463 0.         0.08510463 0.08510463 0.08510463\n",
      "  0.17020925 0.         0.17020925 0.        ]\n",
      " [0.         0.         0.21656487 0.07704384 0.         0.10828244\n",
      "  0.10828244 0.10828244 0.10828244 0.         0.         0.\n",
      "  0.         0.15408769 0.         0.38521922 0.10828244 0.\n",
      "  0.10828244 0.         0.         0.         0.         0.07704384\n",
      "  0.         0.10828244 0.32484731 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.10828244 0.10828244\n",
      "  0.         0.10828244 0.         0.10828244 0.         0.\n",
      "  0.         0.         0.10828244 0.         0.10828244 0.\n",
      "  0.10828244 0.10828244 0.         0.         0.         0.\n",
      "  0.         0.10828244 0.10828244 0.10828244 0.15408769 0.\n",
      "  0.07704384 0.10828244 0.         0.10828244 0.10828244 0.\n",
      "  0.21656487 0.         0.         0.10828244 0.10828244 0.\n",
      "  0.10828244 0.10828244 0.15408769 0.10828244 0.         0.10828244\n",
      "  0.         0.         0.10828244 0.         0.10828244 0.\n",
      "  0.         0.10828244 0.10828244 0.10828244 0.10828244 0.21656487\n",
      "  0.         0.10828244 0.10828244 0.07704384 0.10828244 0.\n",
      "  0.         0.         0.         0.         0.10828244 0.\n",
      "  0.         0.07704384 0.10828244 0.         0.15408769 0.\n",
      "  0.         0.         0.10828244 0.         0.         0.\n",
      "  0.         0.10828244 0.         0.10828244]]\n"
     ]
    }
   ],
   "source": [
    "#Referred from https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html\n",
    "#used methods from TfidTransformer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidfTran = TfidfTransformer(norm=\"l2\")\n",
    "tfidfTran.fit(tf_matrix)\n",
    "tfidf_matrix = tfidfTran.transform(tf_matrix)\n",
    "print(tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.40546511 1.40546511 1.40546511 1.         1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.         1.40546511 1.         1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.         1.40546511\n",
      " 1.         1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.         1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.         1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.         1.40546511 1.40546511 1.         1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidfTran = TfidfTransformer(norm=\"l2\")\n",
    "tfidfTran.fit(tf_matrix)\n",
    "print(tfidfTran.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The idf for terms that appear in one document: 1.916290731874155\n",
      "The idf for terms that appear in two documents: 1.5108256237659907\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def idf(n,df):\n",
    "    result = math.log((n+1.0)/(df+1.0)) + 1\n",
    "    return result\n",
    "print(\"The idf for terms that appear in one document: \" + str(idf(4,1)))\n",
    "print(\"The idf for terms that appear in two documents: \" + str(idf(4,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.08510463 0.17020925 0.         0.06055264 0.08510463 0.\n",
      "  0.         0.         0.         0.08510463 0.08510463 0.08510463\n",
      "  0.08510463 0.06055264 0.08510463 0.48442114 0.         0.08510463\n",
      "  0.         0.17020925 0.17020925 0.08510463 0.08510463 0.06055264\n",
      "  0.08510463 0.         0.         0.17020925 0.08510463 0.08510463\n",
      "  0.08510463 0.08510463 0.08510463 0.08510463 0.         0.\n",
      "  0.08510463 0.         0.08510463 0.         0.08510463 0.08510463\n",
      "  0.08510463 0.08510463 0.         0.08510463 0.         0.08510463\n",
      "  0.         0.         0.08510463 0.08510463 0.08510463 0.08510463\n",
      "  0.08510463 0.         0.         0.         0.12110528 0.08510463\n",
      "  0.06055264 0.         0.08510463 0.         0.         0.08510463\n",
      "  0.         0.08510463 0.08510463 0.         0.         0.25531388\n",
      "  0.         0.         0.06055264 0.         0.08510463 0.\n",
      "  0.08510463 0.08510463 0.         0.17020925 0.         0.08510463\n",
      "  0.08510463 0.         0.         0.         0.         0.\n",
      "  0.08510463 0.         0.         0.24221057 0.         0.08510463\n",
      "  0.08510463 0.08510463 0.08510463 0.08510463 0.         0.08510463\n",
      "  0.08510463 0.06055264 0.         0.08510463 0.06055264 0.08510463\n",
      "  0.08510463 0.08510463 0.         0.08510463 0.08510463 0.08510463\n",
      "  0.17020925 0.         0.17020925 0.        ]\n",
      " [0.         0.         0.21656487 0.07704384 0.         0.10828244\n",
      "  0.10828244 0.10828244 0.10828244 0.         0.         0.\n",
      "  0.         0.15408769 0.         0.38521922 0.10828244 0.\n",
      "  0.10828244 0.         0.         0.         0.         0.07704384\n",
      "  0.         0.10828244 0.32484731 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.10828244 0.10828244\n",
      "  0.         0.10828244 0.         0.10828244 0.         0.\n",
      "  0.         0.         0.10828244 0.         0.10828244 0.\n",
      "  0.10828244 0.10828244 0.         0.         0.         0.\n",
      "  0.         0.10828244 0.10828244 0.10828244 0.15408769 0.\n",
      "  0.07704384 0.10828244 0.         0.10828244 0.10828244 0.\n",
      "  0.21656487 0.         0.         0.10828244 0.10828244 0.\n",
      "  0.10828244 0.10828244 0.15408769 0.10828244 0.         0.10828244\n",
      "  0.         0.         0.10828244 0.         0.10828244 0.\n",
      "  0.         0.10828244 0.10828244 0.10828244 0.10828244 0.21656487\n",
      "  0.         0.10828244 0.10828244 0.07704384 0.10828244 0.\n",
      "  0.         0.         0.         0.         0.10828244 0.\n",
      "  0.         0.07704384 0.10828244 0.         0.15408769 0.\n",
      "  0.         0.         0.10828244 0.         0.         0.\n",
      "  0.         0.10828244 0.         0.10828244]]\n"
     ]
    }
   ],
   "source": [
    "tfidf_matrix = tfidfTran.transform(tf_matrix)\n",
    "print(tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.27058208]\n",
      " [0.27058208 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "cos_similarity_matrix = (tfidf_matrix * tfidf_matrix.T).toarray()\n",
    "print(cos_similarity_matrix)\n",
    "import numpy as np\n",
    "#save the matrix to text file for future use\n",
    "np.savetxt('C:\\\\xampp\\\\htdocs\\\\cyber_project\\\\image_text_cosine_sim.txt' , cos_similarity_matrix)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
